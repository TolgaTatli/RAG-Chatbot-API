import requests
import json
from typing import Dict, Optional
import warnings

# Transformer uyarÄ±larÄ±nÄ± bastÄ±r
warnings.filterwarnings("ignore", category=FutureWarning)
import os
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'


class OllamaRAGQA:
    """Ollama ile yerel LLM kullanarak RAG sistemi"""

    def __init__(self, retriever, model_name: str = "gemma3"):
        """
        Args:
            retriever: RAG retriever instance
            model_name: Ollama model adÄ± (gemma3, llama3.2, deepseek-r1 vb.)
        """
        self.retriever = retriever
        self.model_name = model_name
        self.ollama_url = "http://localhost:11434/api/generate"

    def check_ollama_status(self) -> bool:
        """Ollama'nÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                models = models_data.get('models', [])
                model_names = [model['name'] for model in models]
                print(f"ğŸ“‹ Mevcut modeller: {[name.split(':')[0] for name in model_names]}")

                # Kullanmak istediÄŸimiz modelin var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                current_model_exists = any(self.model_name in name for name in model_names)
                if not current_model_exists:
                    print(f"âš ï¸ Model '{self.model_name}' bulunamadÄ±!")
                    return False

                return True
            return False
        except requests.exceptions.ConnectionError:
            print("âŒ Ollama sunucusuna baÄŸlanÄ±lamadÄ±. 'ollama serve' komutu Ã§alÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ mÄ±?")
            return False
        except Exception as e:
            print(f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {e}")
            return False

    def generate_answer(self, question: str, context: str) -> str:

        if not self.check_ollama_status():
            return "Ollama Ã§alÄ±ÅŸmÄ±yor. LÃ¼tfen 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."

        # Prompt uzunluÄŸunu kontrol et - Ã§ok uzun olabilir
        if len(context) > 15000:
            context = context[:15000] + "..."
            print(f"BaÄŸlam Ã§ok uzun, kÄ±saltÄ±ldÄ±: {len(context)} karakter")

        # Sorunun dilini tespit et
        question_language = "Turkish" if any(turkish_word in question.lower() for turkish_word in ['nedir', 'nasÄ±l', 'neden', 'ne', 'kim', 'hangi']) else "English"
        
        # Yeni hibrit yaklaÅŸÄ±m: RAG bilgilerini kullanarak detaylÄ± ve samimi cevap
        prompt = f"""Sen yardÄ±mcÄ± ve bilgili bir yapay zeka asistanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n sorusunu aÅŸaÄŸÄ±daki bilgileri kullanarak cevaplayacaksÄ±n.

VERÄ° TABANI BÄ°LGÄ°LERÄ°:
{context}

KULLANICININ SORUSU: {question}

TALÄ°MATLAR:
1. Ã–nce verilen bilgileri analiz et
2. Bu bilgileri kullanarak soruya detaylÄ± bir cevap ver
3. CevabÄ±nÄ± samimi ve yardÄ±msever bir tonla yaz
4. EÄŸer bilgi yoksa aÃ§Ä±kÃ§a belirt
5. Ek sorular olursa yardÄ±m etmeye hazÄ±r olduÄŸunu sÃ¶yle
6. {question_language} dilinde cevap ver

LÃ¼tfen soruyu verilen bilgilere dayanarak yanÄ±tla:"""

        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.4,  # Orta seviye yaratÄ±cÄ±lÄ±k
                    "top_p": 0.9,
                    "max_tokens": 600  # Daha uzun cevaplar iÃ§in
                }
            }

            print(f"ğŸ” Ollama API'ye istek gÃ¶nderiliyor...")
            response = requests.post(
                self.ollama_url,
                json=payload,
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'Cevap alÄ±namadÄ±.')
            else:
                error_detail = ""
                try:
                    error_detail = response.json().get("error", "")
                except:
                    error_detail = response.text[:200]

                print(f"âŒ Ollama API hatasÄ±: {response.status_code} - {error_detail}")
                return f"Ollama hatasÄ±: HTTP {response.status_code} - {error_detail}"

        except requests.exceptions.Timeout:
            return "â° Ollama yanÄ±t sÃ¼resi aÅŸÄ±ldÄ±. Model Ã§ok bÃ¼yÃ¼k olabilir."
        except Exception as e:
            print(f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {str(e)}")
            return f"Ollama baÄŸlantÄ± hatasÄ±: {str(e)}"

    def is_general_chat_question(self, question: str) -> bool:
        """Sorunun genel sohbet sorusu olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        general_chat_keywords = [
            'merhaba', 'selam', 'hello', 'hi', 'hey',
            'nasÄ±lsÄ±n', 'nasÄ±l gidiyor', 'how are you', 'how do you do',
            'kimsin', 'who are you', 'what are you', 'ne yapÄ±yorsun',
            'sen kimsin', 'sen nesin', 'yapay zeka', 'artificial intelligence',
            'bot musun', 'robot musun', 'ai misin', 'asistan mÄ±sÄ±n',
            'teÅŸekkÃ¼r', 'thank you', 'thanks', 'saÄŸol', 'merci',
            'hoÅŸÃ§a kal', 'bye', 'goodbye', 'gÃ¶rÃ¼ÅŸÃ¼rÃ¼z', 'see you',
            'yardÄ±m et', 'help me', 'yardÄ±m', 'help', 'nasÄ±l yardÄ±m',
            'anlat', 'tell me about yourself', 'kendin hakkÄ±nda',
            'iyi misin', 'are you ok', 'how you doing'
        ]
        
        question_lower = question.lower()
        
        # Spesifik bilgi sorularÄ±: "X nedir?" formatÄ± - RAG moduna gitsin
        if ('nedir' in question_lower or 'what is' in question_lower) and len(question.split()) > 1:
            # Sadece "nedir?" tek baÅŸÄ±na deÄŸilse RAG moduna git
            if question_lower.strip() not in ['nedir?', 'what is?']:
                return False
        
        # Tam eÅŸleÅŸme kontrolÃ¼ (sadece Ã§ok genel sorular iÃ§in)
        exact_matches = [
            'kimsin?', 'ne yapÄ±yorsun?', 'yardÄ±m?',
            'sen kimsin?', 'sen nesin?', 'sen ne yapÄ±yorsun?',
            'yapay zeka mÄ±sÄ±n?', 'bot musun?', 'ai misin?',
            'merhaba?', 'selam?', 'nedir?'
        ]
        
        if question_lower in exact_matches:
            return True
            
        return any(keyword in question_lower for keyword in general_chat_keywords)

    def generate_general_response(self, question: str) -> str:
        """Genel sohbet sorularÄ± iÃ§in yapay zeka benzeri cevap Ã¼ret"""
        if not self.check_ollama_status():
            return "Ollama Ã§alÄ±ÅŸmÄ±yor. LÃ¼tfen 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."

        prompt = f"""Sen yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. KullanÄ±cÄ±nÄ±n sorusuna doÄŸal ve samimi bir ÅŸekilde cevap ver.

SORU: {question}

KÄ±sa, samimi ve yardÄ±msever bir cevap ver. TÃ¼rkÃ§e cevap ver."""

        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,  # Daha yaratÄ±cÄ± yanÄ±tlar iÃ§in
                    "top_p": 0.9,
                    "max_tokens": 150
                }
            }

            response = requests.post(
                self.ollama_url,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'Cevap alÄ±namadÄ±.')
            else:
                return "ÃœzgÃ¼nÃ¼m, ÅŸu anda cevap veremiyorum."

        except Exception as e:
            return "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"

    def answer_question(self, question: str, top_k: int = 3, confidence_threshold: float = 0.2) -> Dict:
        # Ã–nce genel sohbet sorusu mu kontrol et
        if self.is_general_chat_question(question):
            answer = self.generate_general_response(question)
            return {
                'question': question,
                'answer': answer,
                'context': "",
                'sources': [],
                'method': 'general_chat'
            }

        # RAG aramasÄ± yap
        search_results = self.retriever.search(question, top_k + 2)  # Daha fazla sonuÃ§ al, filtreleme sonrasÄ± iÃ§in

        # SonuÃ§ yoksa
        if not search_results:
            return {
                'question': question,
                'answer': "ÃœzgÃ¼nÃ¼m, bu soruyla ilgili bilgi bulamadÄ±m.",
                'context': "",
                'sources': [],
                'method': 'no_results'
            }

        # SonuÃ§larÄ± Ã§eÅŸitlendirmek iÃ§in, aynÄ± iÃ§erikleri filtrele
        filtered_results = []
        seen_texts = set()

        for result in search_results:
            # Ä°lk 50 karakter benzersiz mi kontrol et
            text_signature = result['text'][:50]
            if text_signature not in seen_texts:
                seen_texts.add(text_signature)
                filtered_results.append(result)
                if len(filtered_results) >= top_k:
                    break

        # FiltrelenmiÅŸ sonuÃ§larÄ± kullan
        search_results = filtered_results[:top_k]

        # BaÄŸlam oluÅŸtur
        context = self.retriever.get_context_for_query(question, top_k)

        # En yÃ¼ksek gÃ¼ven skoru
        top_confidence = search_results[0]['score'] if search_results else 0

        # GÃ¼ven skoru dÃ¼ÅŸÃ¼kse
        low_confidence = top_confidence < confidence_threshold

        # Ollama ile cevap oluÅŸtur
        if self.check_ollama_status():
            # GÃ¼ven skoru dÃ¼ÅŸÃ¼kse uyarÄ± ekle
            if low_confidence:
                answer = self.generate_answer(question, context)
                answer = f"âš ï¸ GÃ¼ven skoru dÃ¼ÅŸÃ¼k. Cevap doÄŸru olmayabilir.\n\n{answer}"
                method = 'low_confidence'
            else:
                answer = self.generate_answer(question, context)
                method = 'ollama_generated'
        else:
            # Fallback: En iyi eÅŸleÅŸen dÃ¶kÃ¼manÄ± kullan
            best_match = search_results[0]
            answer = f"ğŸ” En ilgili bilgi: {best_match['text'][:300]}..."
            method = 'retrieval_only'

        return {
            'question': question,
            'answer': answer,
            'context': context,
            'sources': search_results,
            'confidence': top_confidence,
            'method': method
        }

    def interactive_qa(self):
        """EtkileÅŸimli soru-cevap modu"""
        print("ğŸ¦™ Ollama RAG Soru-Cevap Sistemi")
        print(f"Model: {self.model_name}")
        print("Ã‡Ä±kmak iÃ§in 'quit' yazÄ±n.")
        print("-" * 50)

        # Ollama durumunu kontrol et
        if not self.check_ollama_status():
            print("âš ï¸  Ollama Ã§alÄ±ÅŸmÄ±yor!")
            print("Kurulum iÃ§in: https://ollama.ai")
            print("BaÅŸlatmak iÃ§in: ollama serve")
            print("Model indirmek iÃ§in: ollama deepseek-r1")
            print("\nYine de temel arama yapabilirsiniz...")
        else:
            print(f"âœ… Ollama aktif - Model: {self.model_name}")

        while True:
            question = input("\nâ“ Sorunuz: ").strip()

            # BoÅŸ sorgu kontrolÃ¼
            if not question:
                print("LÃ¼tfen bir soru yazÄ±n.")
                continue

            # Ã‡Ä±kÄ±ÅŸ kontrolÃ¼
            if question.lower() in ['quit', 'exit', 'Ã§Ä±k', 'Ã§Ä±kÄ±ÅŸ']:
                print("ğŸ‘‹ HoÅŸÃ§a kalÄ±n!")
                break

            try:
                print("ğŸ” AranÄ±yor...")
                result = self.answer_question(question)

                print(f"\nğŸ’¬ Cevap:")
                print(result['answer'])

                if result['sources']:
                    print(f"\nğŸ“Š GÃ¼ven skoru: {result['confidence']:.3f}")
                    print(f"ğŸ“ Kaynak sayÄ±sÄ±: {len(result['sources'])}")
                    print(f"ğŸ”§ Method: {result['method']}")

                    if len(result['sources']) > 0:
                        print(f"\nğŸ“š Kaynaklar:")
                        for i, source in enumerate(result['sources'][:2]):
                            print(f"  {i+1}. {source['text'][:100]}... (skor: {source['score']:.3f})")
                elif result['method'] == 'general_chat':
                    print("ğŸ’¬ Genel sohbet modu")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ KullanÄ±cÄ± tarafÄ±ndan durduruldu!")
                break
            except Exception as e:
                print(f"\nâŒ Hata oluÅŸtu: {str(e)}")
                print(f"âŒ Hata tÃ¼rÃ¼: {type(e).__name__}")
                import traceback
                print(f"âŒ DetaylÄ± hata:")
                traceback.print_exc()
                print("Tekrar deneyin veya 'quit' yazarak Ã§Ä±kÄ±n.")


if __name__ == "__main__":
    from rag_system import RAGRetriever

    print("Ollama RAG sistemi baÅŸlatÄ±lÄ±yor...")

    retriever = RAGRetriever()

    try:
        retriever.load_from_files("faiss_index.bin", "documents.pkl")
        qa_system = OllamaRAGQA(retriever, model_name="gemma3")
        qa_system.interactive_qa()

    except FileNotFoundError:
        print("âŒ Index dosyalarÄ± bulunamadÄ±. Ã–nce data_processor.py Ã§alÄ±ÅŸtÄ±rÄ±n.")
    except Exception as e:
        print(f"âŒ Hata: {e}")
