from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Dict

class HuggingFaceRAGQA:
    """Hugging Face Transformers ile Ã¼cretsiz RAG sistemi"""

    def __init__(self, retriever, model_name: str = "microsoft/DialoGPT-medium"):
        """
        Args:
            retriever: RAG retriever instance
            model_name: HF model adÄ±
        """
        self.retriever = retriever
        self.model_name = model_name
        self.generator = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # KÃ¼Ã§Ã¼k modeller (RAM dostu)
        self.small_models = {
            "turkish": "microsoft/DialoGPT-small",  # KÃ¼Ã§Ã¼k model
            "multilingual": "microsoft/DialoGPT-medium",  # Orta model
            "code": "Salesforce/codegen-350M-mono",  # Kod iÃ§in
            "chat": "microsoft/DialoGPT-large"  # BÃ¼yÃ¼k model (daha iyi ama yavaÅŸ)
        }

    def load_model(self):
        """Modeli yÃ¼kle"""
        try:
            print(f"ğŸ¤– Model yÃ¼kleniyor: {self.model_name}")
            print("â³ Bu iÅŸlem biraz zaman alabilir...")

            self.generator = pipeline(
                "text-generation",
                model=self.model_name,
                device=0 if self.device == "cuda" else -1,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                trust_remote_code=True
            )

            print(f"âœ… Model baÅŸarÄ±yla yÃ¼klendi ({self.device})")
            return True

        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print("ğŸ’¡ Ä°nternetsiz alternatif modeller deneyin")
            return False

    def generate_answer(self, question: str, context: str) -> str:
        """
        HuggingFace model ile cevap Ã¼ret
        """
        if not self.generator:
            if not self.load_model():
                return "Model yÃ¼klenemedi. LÃ¼tfen model adÄ±nÄ± kontrol edin."

        prompt = f"""BaÄŸlam: {context[:800]}

Soru: {question}

Cevap:"""

        try:
            response = self.generator(
                prompt,
                max_length=len(prompt.split()) + 100,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.generator.tokenizer.eos_token_id
            )

            # Sadece yeni Ã¼retilen kÄ±smÄ± al
            generated_text = response[0]['generated_text']
            answer = generated_text[len(prompt):].strip()

            if not answer:
                return "ÃœzgÃ¼nÃ¼m, bu soruya cevap oluÅŸturamadÄ±m."

            return answer

        except Exception as e:
            return f"Cevap Ã¼retme hatasÄ±: {e}"

    def answer_question(self, question: str, top_k: int = 3) -> Dict:
        """Soruya cevap ver"""
        search_results = self.retriever.search(question, top_k)
        context = self.retriever.get_context_for_query(question, top_k)

        if not search_results:
            return {
                'question': question,
                'answer': "Bu soruyla ilgili bilgi bulamadÄ±m.",
                'context': "",
                'sources': [],
                'method': 'no_results'
            }

        if self.generator or self.load_model():
            answer = self.generate_answer(question, context)
            method = 'huggingface_generated'
        else:
            best_match = search_results[0]
            answer = f"En ilgili bilgi: {best_match['text'][:300]}..."
            method = 'retrieval_only'

        return {
            'question': question,
            'answer': answer,
            'context': context,
            'sources': search_results,
            'confidence': search_results[0]['score'] if search_results else 0,
            'method': method
        }

    def interactive_qa(self):
        """EtkileÅŸimli soru-cevap"""
        print("ğŸ¤— Hugging Face RAG Sistemi")
        print(f"Model: {self.model_name}")
        print(f"Cihaz: {self.device}")
        print("-" * 50)

        while True:
            question = input("\nâ“ Sorunuz: ").strip()

            if question.lower() in ['quit', 'Ã§Ä±k', 'exit', 'q']:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break

            if not question:
                continue

            try:
                print("ğŸ” AranÄ±yor ve cevap Ã¼retiliyor...")
                result = self.answer_question(question)

                print(f"\nâœ… **Cevap:**")
                print(result['answer'])

                print(f"\nğŸ“Š **Bilgiler:**")
                print(f"â€¢ YÃ¶ntem: {result['method']}")
                print(f"â€¢ GÃ¼ven: {result.get('confidence', 0):.3f}")

            except Exception as e:
                print(f"âŒ Hata: {e}")

if __name__ == "__main__":
    from rag_system import RAGRetriever

    # Retriever'Ä± baÅŸlat
    retriever = RAGRetriever()

    try:
        retriever.load_from_files("faiss_index.bin", "documents.pkl")

        # HuggingFace QA sistemi oluÅŸtur
        qa_system = HuggingFaceRAGQA(retriever)
        qa_system.interactive_qa()

    except FileNotFoundError:
        print("âŒ Index dosyalarÄ± bulunamadÄ±.")
    except Exception as e:
        print(f"âŒ Hata: {e}")
